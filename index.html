<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="description" content="OATH: Adaptive Obstacle-Aware Task Assignment and Planning for Heterogeneous Robot Teaming.">
  <meta name="keywords" content="OATH, Multi-Agent Task Assignment and Planning, MATP, Heterogeneous Robot Teaming, LLM, Robotics">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>OATH: Adaptive Obstacle-Aware Task Assignment and Planning for Heterogeneous Robot Teaming</title>

  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&family=Playfair+Display:wght@600;700;800&display=swap" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.jpeg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- ====== HERO: Full-screen video background ====== -->
<section class="hero-video-section">
  <div class="hero-video-bg">
    <video autoplay muted loop playsinline>
      <source src="./static/videos/bg_video.mp4" type="video/mp4">
    </video>
    <div class="hero-video-overlay"></div>
  </div>
  <div class="hero-video-content">
    <h1 class="hero-title">
      Adaptive <span class="hero-highlight">O</span>bstacle-<span class="hero-highlight">A</span>ware
      <span class="hero-highlight">T</span>ask Assignment and Planning<br>
      for <span class="hero-highlight">H</span>eterogeneous Robot Teaming
    </h1>
    <p class="hero-acronym">OATH</p>
  </div>
  <div class="hero-scroll-hint"><span></span></div>
</section>

<!-- ====== Paper Info Bar ====== -->
<section class="paper-info-bar">
  <div class="container is-max-desktop">
    <div class="paper-info-content">
      <h2 class="paper-info-title">
        <span class="accent">OATH</span>: Adaptive <span class="accent">O</span>bstacle-<span class="accent">A</span>ware
        <span class="accent">T</span>ask Assignment and Planning
        for <span class="accent">H</span>eterogeneous Robot Teaming
      </h2>
      <p class="paper-info-authors">Anonymous Authors</p>
      <!-- <p class="paper-info-authors">
        Nan Li<sup>1</sup>, Jiming Ren<sup>2</sup>, Haris Miller<sup>2</sup>,
        Samuel Coogan<sup>2</sup>, Karen M. Feigh<sup>2</sup>, Ye Zhao<sup>1,2</sup>
      </p>
      <p class="paper-info-affiliations">
        <sup>1</sup>Institute for Robotics and Intelligent Machines (IRIM), Georgia Institute of Technology &nbsp;&middot;&nbsp;
        <sup>2</sup>College of Computing, Georgia Institute of Technology
      </p> -->
    </div>
  </div>
</section>

<!-- ====== Overview Video ====== -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="content-column">
      <div class="video-wrapper">
        <video autoplay controls muted loop playsinline>
          <source src="./static/videos/OATH_compressed.mp4" type="video/mp4">
        </video>
      </div>
      <p class="video-caption">
        <strong>Overview of OATH</strong> &mdash; a novel obstacle-aware multi-agent task assignment and planning framework.
      </p>
    </div>
  </div>
</section>

<!-- ====== Abstract ====== -->
<section class="section section-alt">
  <div class="container is-max-desktop">
    <div class="content-column">
      <h2 class="section-heading">Abstract</h2>
      <div class="text-block">
        <p>
          Multi-Agent Task Assignment and Planning (MATP) has attracted growing attention but remains challenging in terms of scalability, spatial reasoning, and adaptability in obstacle-rich environments.
        </p>
        <p>
          To address these challenges, we propose <strong>OATH</strong> (Adaptive Obstacle-Aware Task Assignment and Planning for Heterogeneous Robot Teaming).
          Our framework advances MATP by introducing two novel obstacle-aware strategies for task allocation. First, we develop an adaptive Halton sequence map&mdash;the first known application of Halton sampling with obstacle-aware adaptation in MATP&mdash;which adjusts sampling density based on obstacle distribution. Second, we integrate Dijkstra task-to-task distance matrices that encode traversability. Combined, these strategies significantly improve allocation quality in obstacle-rich environments.
          For task assignment, we propose a cluster&ndash;auction&ndash;selection framework that integrates obstacle-aware clustering with weighted auctions and intra-cluster task selection. These mechanisms jointly enable effective coordination among heterogeneous robots while maintaining scalability and near-optimal allocation performance.
          In addition, our framework leverages an LLM to interpret human instructions and directly guide the planner in real time.
        </p>
        <p>
          We validate <strong>OATH</strong> in both NVIDIA Isaac Sim and real-world hardware experiments using TurtleBot platforms, demonstrating substantial improvements in task assignment quality, scalability, adaptability to dynamic changes, and overall execution performance compared to state-of-the-art MATP baselines.
        </p>
      </div>
    </div>
  </div>
</section>

<!-- ====== Framework Overview ====== -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="content-column">
      <h2 class="section-heading">Framework Overview</h2>
      <figure class="figure-block">
        <img src="./static/images/new_framework_loop.png" alt="Overview of the OATH framework.">
        <figcaption>Architecture of the OATH framework with adaptive obstacle-aware task allocation pipeline.</figcaption>
      </figure>
    </div>
  </div>
</section>



<!-- ====== Simulation Results ====== -->
<section class="section section-alt">
  <div class="container is-max-desktop">
    <div class="content-column">
      <h2 class="section-heading">Obstacle-Aware Multi-Agent Pickup and Delivery</h2>

      <h3 class="subsection-heading">Task Assignment Results</h3>
      <p class="text-body">
        Comparison of heterogeneous task assignment results under different numbers of task types.
        Each subfigure illustrates the final task assignment for teams operating with 2, 3, or 5 task types, respectively.
      </p>
      <figure class="figure-block">
        <img src="./static/images/heterogenous_comparison.png" alt="Heterogeneous task assignment comparison.">
      </figure>

      <h3 class="subsection-heading">Simulation Visualization</h3>
      <p class="text-body">
        Simulation with two ground robots and two drones in Isaac Sim, where the ground robots handle both red and blue tasks while the drones only handle blue tasks.
        The exclamation marks indicate the task locations.
      </p>
      <div class="video-wrapper">
        <video autoplay controls muted loop playsinline>
          <source src="./static/videos/Isaac_Sim.mp4" type="video/mp4">
        </video>
      </div>

      <h3 class="subsection-heading">Heterogeneous Speed Profiles</h3>
      <p class="text-body">
        Our framework also accounts for differences in kinematic and dynamic characteristics among heterogeneous robots, such as varying speeds.
      </p>
      <div class="case-studies" style="grid-template-columns:1fr;">
        <div class="case-card" style="max-width:480px;margin:0 auto;">
          <h3 class="case-title">Case: Different Robot Speeds</h3>
          <div class="video-wrapper">
            <video autoplay controls muted loop playsinline>
              <source src="./static/videos/different speed.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ====== LLM Integration ====== -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="content-column">
      <h2 class="section-heading">LLM as the Translator of Different Contexts</h2>
      <p class="text-body">
        A fully integrated online pipeline with LLM-guided interaction for multi-robot systems.
        It interprets natural language inputs and supports real-time replanning in response to human instructions.
      </p>
      <figure class="figure-block">
        <img src="./static/images/LLM_structure.png" alt="LLM integration pipeline structure.">
        <figcaption>LLM-guided interaction pipeline for real-time multi-robot replanning.</figcaption>
      </figure>

      <h3 class="subsection-heading">LLM-Guided Replanning &mdash; Case Studies</h3>
      <div class="case-studies">
        <div class="case-card">
          <h3 class="case-title">Case 1: Add New Tasks</h3>
          <div class="video-wrapper">
            <video autoplay controls muted loop playsinline>
              <source src="./static/videos/LLM-Tasks.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div class="case-card">
          <h3 class="case-title">Case 2: New Obstacles Detected</h3>
          <div class="video-wrapper">
            <video autoplay controls muted loop playsinline>
              <source src="./static/videos/LLM-obstacle.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div class="case-card">
          <h3 class="case-title">Case 3: Change Task Priority</h3>
          <div class="video-wrapper">
            <video autoplay controls muted loop playsinline>
              <source src="./static/videos/LLM-priority.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ====== Hardware Experiments (PRIORITY) ====== -->
<section class="section section-alt">
  <div class="container is-max-desktop">
    <div class="content-column">
      <h2 class="section-heading">Hardware Experiments</h2>
      <p class="text-body">
        We validate the OATH framework on physical robot platforms to demonstrate real-world applicability.
        The following videos show hardware experiments with heterogeneous robot teams executing obstacle-aware task assignments.
      </p>

      <div class="hw-experiment">
        <h3 class="subsection-heading">Multi-Robot Hardware Experiment</h3>
        <div class="video-wrapper">
          <video autoplay controls muted loop playsinline>
            <source src="./static/videos/Hardware Experiment.mp4" type="video/mp4">
          </video>
        </div>
      </div>

      <div class="hw-experiment">
        <h3 class="subsection-heading">Hardware Experiment with LLM Integration</h3>
        <div class="video-wrapper">
          <video autoplay controls muted loop playsinline>
            <source src="./static/videos/Hardware_experiment_with_LLM.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ====== Quantitative Results ====== -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="content-column">
      <h2 class="section-heading">Quantitative Results</h2>

      <p class="text-body">
        We compare <strong>OATH</strong> against three baselines:
        <strong>CBBA</strong> (consensus-based bundle algorithm),
        <strong>KAN</strong> (<em>k</em>-means clustering + auction + nearest-neighbor heuristic with Euclidean distances), and
        <strong>KAM</strong> (<em>k</em>-means clustering + auction + MILP-based intra-cluster solver with Dijkstra distances).
        All variants share the same path planning algorithm.
      </p>

      <!-- Table 1: Method Comparison -->
      <h3 class="subsection-heading">Method Overview</h3>
      <div class="table-container">
        <table class="academic-table">
          <caption>Table 1. Comparison of methods evaluated. All variants use the same path planning algorithm.</caption>
          <thead>
            <tr>
              <th>Method</th>
              <th>Clustering</th>
              <th>Intra-Cluster</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>CBBA</td>
              <td>Bundle</td>
              <td>Bundle Order</td>
            </tr>
            <tr>
              <td>KAN</td>
              <td><em>k</em>-means</td>
              <td>NN</td>
            </tr>
            <tr>
              <td>KAM</td>
              <td><em>k</em>-means</td>
              <td>MILP</td>
            </tr>
            <tr class="highlight-row">
              <td><strong>OATH (ours)</strong></td>
              <td>Obstacle-Aware Cluster</td>
              <td>MILP</td>
            </tr>
          </tbody>
        </table>
      </div>

      <!-- Table 3: Total Steps & Running Time -->
      <h3 class="subsection-heading">Total Steps &amp; Running Time</h3>
      <div class="table-container">
        <table class="academic-table">
          <caption>Table 3. Total steps and total running time (s) vs. task number.</caption>
          <thead>
            <tr>
              <th colspan="5" class="table-group-header">Total Steps</th>
            </tr>
            <tr>
              <th>Tasks</th>
              <th>CBBA</th>
              <th>KAN</th>
              <th>KAM</th>
              <th>OATH</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>10</td>
              <td>314</td>
              <td>332</td>
              <td>330</td>
              <td class="best"><strong>222</strong></td>
            </tr>
            <tr>
              <td>15</td>
              <td>667</td>
              <td>451</td>
              <td>456</td>
              <td class="best"><strong>351</strong></td>
            </tr>
            <tr>
              <td>20</td>
              <td>773</td>
              <td>555</td>
              <td>510</td>
              <td class="best"><strong>463</strong></td>
            </tr>
            <tr>
              <td>25</td>
              <td>954</td>
              <td>654</td>
              <td>608</td>
              <td class="best"><strong>562</strong></td>
            </tr>
            <tr>
              <td>30</td>
              <td>1031</td>
              <td>820</td>
              <td>802</td>
              <td class="best"><strong>768</strong></td>
            </tr>
          </tbody>
          <thead>
            <tr>
              <th colspan="5" class="table-group-header">Total Running Time (s)</th>
            </tr>
            <tr>
              <th>Tasks</th>
              <th>CBBA</th>
              <th>KAN</th>
              <th>KAM</th>
              <th>OATH</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>10</td>
              <td>68.53</td>
              <td>54.79</td>
              <td>54.60</td>
              <td class="best"><strong>40.36</strong></td>
            </tr>
            <tr>
              <td>15</td>
              <td>94.20</td>
              <td>84.13</td>
              <td>83.30</td>
              <td class="best"><strong>60.11</strong></td>
            </tr>
            <tr>
              <td>20</td>
              <td>106.02</td>
              <td>90.66</td>
              <td>85.96</td>
              <td class="best"><strong>76.98</strong></td>
            </tr>
            <tr>
              <td>25</td>
              <td>111.96</td>
              <td>95.02</td>
              <td>95.35</td>
              <td class="best"><strong>94.76</strong></td>
            </tr>
            <tr>
              <td>30</td>
              <td>124.94</td>
              <td>113.14</td>
              <td>129.95</td>
              <td class="best"><strong>103.63</strong></td>
            </tr>
          </tbody>
        </table>
      </div>

      <!-- Figure: Experiment_4_method -->
      <figure class="figure-block figure-compact">
        <img src="./static/images/Experiment_4_method.png" alt="Total running steps and time vs. task number." style="max-width:420px;width:100%;">
        <figcaption>Figure 1. Total running steps (top) and total running time (bottom) of the four robots as a function of task number.</figcaption>
      </figure>

      <!-- Table 4: OATH vs. MILP -->
      <h3 class="subsection-heading">Comparison with MILP Baseline</h3>
      <div class="table-container">
        <table class="academic-table">
          <caption>Table 4. Comparison between OATH and the MILP baseline (Gurobi) in total execution distance and computation time.</caption>
          <thead>
            <tr>
              <th rowspan="2">Tasks</th>
              <th colspan="4">Total Distance</th>
              <th colspan="2">Time (s)</th>
            </tr>
            <tr>
              <th>OATH</th>
              <th>MILP</th>
              <th>Gap to MILP (%)</th>
              <th>Gap to LB (%)</th>
              <th>OATH</th>
              <th>MILP</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>9</td>
              <td>88.108</td>
              <td>67.043</td>
              <td>23.91</td>
              <td>0.00</td>
              <td class="best"><strong>0.0716</strong></td>
              <td>3.96</td>
            </tr>
            <tr>
              <td>12</td>
              <td>102.446</td>
              <td>85.381</td>
              <td>16.66</td>
              <td>0.00</td>
              <td class="best"><strong>0.0837</strong></td>
              <td>32.94</td>
            </tr>
            <tr>
              <td>15</td>
              <td>125.051</td>
              <td>108.226</td>
              <td>13.45</td>
              <td>6.07</td>
              <td class="best"><strong>0.2011</strong></td>
              <td>&ge;600</td>
            </tr>
            <tr>
              <td>18</td>
              <td>146.529</td>
              <td>133.006</td>
              <td>9.23</td>
              <td>16.69</td>
              <td class="best"><strong>0.3237</strong></td>
              <td>&ge;600</td>
            </tr>
            <tr>
              <td>21</td>
              <td>208.357</td>
              <td>155.572</td>
              <td>25.33</td>
              <td>22.12</td>
              <td class="best"><strong>0.3663</strong></td>
              <td>&ge;600</td>
            </tr>
          </tbody>
        </table>
      </div>

      <!-- Figure: Scalability -->
      <h3 class="subsection-heading">Scalability</h3>
      <figure class="figure-block figure-compact">
        <img src="./static/images/Scalability_Result.png" alt="Scalability of OATH with increasing task number." style="max-width:480px;width:100%;">
        <figcaption>Figure 2. Scalability of OATH with respect to the number of tasks. With the team and planner fixed, increasing P from 25 to 100 yields approximately linear growth in total steps (left axis) and total time (right axis).</figcaption>
      </figure>

    </div>
  </div>
</section>

<!-- ====== LLM User Study ====== -->
<section class="section section-alt">
  <div class="container is-max-desktop">
    <div class="content-column">
      <h2 class="section-heading">LLM-Guided Instruction User Study</h2>
      <p class="text-body">
        In addition to evaluating intent parsing accuracy and system performance impact, we conduct a user study to quantify human-in-the-loop efficiency and analyze the latency of the LLM-guided pipeline.
      </p>

      <h3 class="subsection-heading">Study Design</h3>
      <p class="text-body">
        We recruit eight participants to evaluate the efficiency of human interventions. Each participant performs three trials for each intervention intention, resulting in 24 trials per intention. Participants are allowed to freely design their own instructions in each trial&mdash;they may introduce new tasks or new obstacles at arbitrary locations in the environment, without being restricted to predefined templates or fixed coordinates.
      </p>
      <p class="text-body">
        To evaluate the efficiency and latency characteristics of the LLM-guided pipeline, we conduct two complementary experiments. The first experiment examines the overall human-in-the-loop interaction by measuring human formulation time and total system reaction time. The second experiment further decomposes the system reaction time into LLM inference time and planner update time, allowing us to analyze the source of latency under different intervention types.
      </p>

      <h3 class="subsection-heading">Results</h3>
      <div class="figure-row">
        <figure class="figure-block">
          <img src="./static/images/LLM_experiment_results.png" alt="LLM user study experiment results." style="width:360px;">
          <figcaption>Human formulation time and total system reaction time across intervention types.</figcaption>
        </figure>
        <figure class="figure-block">
          <img src="./static/images/LLM_System_time_analysis.png" alt="LLM system time analysis." style="width:360px;">
          <figcaption>Decomposition of system reaction time into LLM inference time and planner update time.</figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>

<!-- ====== Conclusion ====== -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="content-column">
      <h2 class="section-heading">Conclusion</h2>
      <div class="text-block">
        <p>
          <strong>OATH</strong> introduces an adaptive Halton sequence that dynamically adjusts sampling density based on obstacle distribution.
          In addition, the proposed hierarchical cluster&ndash;auction&ndash;task selection scheme generalizes to any number of task types and reduces allocation complexity while respecting robot capacity and capability constraints.
          Together, these components enable scalable and near-optimal task assignment for heterogeneous robot teams operating in obstacle-rich environments.
        </p>
        <p>
          Beyond task assignment, <strong>OATH</strong> integrates LLMs as persistent interpreters throughout the execution phase.
          Unlike prior approaches that employ LLMs only for initialization, our framework continuously leverages LLMs to translate natural language instructions into structured constraints and task updates.
          This design ensures ongoing adaptability to dynamic human intent, unforeseen obstacles, and mission changes.
        </p>
      </div>
    </div>
  </div>
</section>

<!-- ====== Footer ====== -->
<footer class="site-footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8 has-text-centered">
        <p class="footer-text">
          This website is licensed under a
          <a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="license">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          Website template adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
        </p>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
